# **强化学习笔记**

> Writted on 250227

> Author:Jack_Hui

## **强化学习概述**
强化学习属于机器学习的一种。机器学习可以大体分为监督学习、无监督学习、强化学习以及深度学习四类。对于监督学习，训练前需要给出标注好的数据，比如在分类任务中，训练数据集需要给出每个图片对应的分类；对于无监督学习，给出的数据集并不需要进行标注，由机器按照特定算法总结特征，可理解为自学成才。而强化学习重点聚焦于与环境的交互过程，训练数据集并没有提前人为给出，而是通过与环境的一系列交互获得，再根据人为设置的奖励机制决定交互过程。最后深度学习涉猎最广泛，可以与前三者结合，在不同的领域上发挥更好的效果。

## **马尔科夫决策过程**
通常情况下可以使用马尔科夫决策过程对强化学习场景进行建模，该决策的一个重要特质是未来状态只受当前状态影响，公式表示如下：

$$
\emph{P}(\emph{S}_\emph{t+1}|\emph{S}_\emph{t}) = \emph{P}(\emph{S}_\emph{t+1}|\emph{S}_\emph{1},\emph{S}_\emph{2},...,\emph{S}_\emph{t})
$$

此外，强化学习过程中还设置了奖励函数$\emph{R}$以及定义了$\emph{t}$时刻根据某一决策获得的累积奖励$\emph{G}_\emph{t}$：

$$
\emph{G}_\emph{t} = \emph{R}_ \emph{t+1}+\gamma * \emph{R}_ \emph{t+2}+...+\gamma^k * \emph{R}_ \emph{t+k+1}
$$

当然，在时刻$\emph{t}$可能会有多种决策，计算不同决策下产生的$\emph{G}_\emph{t}$，得到一个期望，这样就得出马尔科夫决策过程中的**状态-价值函数**，即：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{G}_\emph{t}|\emph{S}_\emph{t} = \emph{s}]
$$

将$\emph{V}_\emph{s}$进一步展开，可以发现：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{R}_ \emph{t+1}] + \gamma *  \emph{E}[\emph{G}_\emph{t+1}|\emph{S}_\emph{t+1} = \emph{s}']
$$

需要注意的是，$\emph{s}'$可能有多种情况，所以公式可改写为：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{R}_ \emph{t+1}] + \gamma *  \sum_{\emph{s}' \in \emph{S}} \emph{P}_{\emph{s}\emph{s}'}*\emph{V}_{\emph{s}'} 
$$

在此基础之上，我们可以进一步进行改写，并给定决策过程$\pi$，得到**状态-价值函数**与**行为-价值函数**之间的关系：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)\sum_r P(r|s,a)*r+\gamma*\sum_a \pi(a|s) \sum_{s'} P(s'|s,a)*V_{s'}
$$

再定义**行为-价值函数**$Q_{\pi}(s,a)$：

$$
Q_{\pi}(s,a) = \sum_r P(r|s,a)*r + \gamma* \sum_{s'} P(s'|s,a)*V_{s'}
$$

则可以得到著名的**贝尔曼方程**表达式：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)*Q_{\pi}(s,a) 
$$

不难发现，该公式表明在某一个决策过程中，特定状态的价值可由该状态下可能采取的行动价值加权得到，而行动价值又由奖励函数以及下一个可能到达的状态的价值共同确定

以上推倒过程涉及到了**奖励函数**$R$、**状态空间**$S$、**动作空间**$A$、**状态转移概率**$P(s'|s,a)$以及**折扣因子**$\gamma$，因此马尔科夫决策过程可用该五元组进行表示。

## **Q-Leanring算法**
强化学习价值函数更新公式可以理解为：**即时奖励**+折扣因子***下一个状态价值**，
$$
Q(s,a) = R(s,a)+γ*max_{a'}Q(s',a') 
$$
在q-Leanring算法中，即时奖励由预先定义好的奖励机制给出，下一个状态价值则需要通过Q-Table迭代收敛得到，由于需要打表，当状态空间趋于无穷时，该方法不再适用，但其Q值迭代公式仍然值得参考。

## **DQN算法**
DQN算法是对Q-Learning算法的改进，其核心是使用神经网络来代替Q-Table，从而实现对Q值的迭代更新。神经网络拟合过程可以用如下公式通俗理解：

$$
预测新值 = 预测旧值 + 学习率*(真实值-预测旧值) 
$$

带入到DQN算法场景下，可以得到如下迭代 公式：

$$
Q(s,a) =Q(s,a)+\alpha *(R(s,a)+γ*max_{a'}Q(s',a')-Q(s,a))
$$

这里真实值即为Q-Leanring算法中迭代公式计算得到的结果，损失函数也可通过真实值与预测值的均方根误差来表达。

此外，DQN算法还创新性的引入了**经验回放**以及**目标网络**的概念，一方面，通过经验回放的随机抽取，可以打乱数据样本之间的强关联性；另一方面，目标网络避免了价值迭代公式两边同时发生变化，保证了收敛过程的稳定。

DQN可以很好地解决连续状态空间的问题，但不能处理连续动作空间，因为DQN网络输出的是一个Q值，当动作空间离散时，尚可通过比较取最大值；但当动作空间为连续时，无法通过比较取最大值。