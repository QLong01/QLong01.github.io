# **强化学习笔记**

> Writted on 250227

> Author:Jack_Hui

## **强化学习概述**
强化学习属于机器学习的一种。机器学习可以大体分为监督学习、无监督学习、强化学习以及深度学习四类。对于监督学习，训练前需要给出标注好的数据，比如在分类任务中，训练数据集需要给出每个图片对应的分类；对于无监督学习，给出的数据集并不需要进行标注，由机器按照特定算法总结特征，可理解为自学成才。而强化学习重点聚焦于与环境的交互过程，训练数据集并没有提前人为给出，而是通过与环境的一系列交互获得，再根据人为设置的奖励机制决定交互过程。最后深度学习涉猎最广泛，可以与前三者结合，在不同的领域上发挥更好的效果。

## **马尔科夫决策过程**
通常情况下可以使用马尔科夫决策过程对强化学习场景进行建模，该决策的一个重要特质是未来状态只受当前状态影响，公式表示如下：

$$
\emph{P}(\emph{S}_\emph{t+1}|\emph{S}_\emph{t}) = \emph{P}(\emph{S}_\emph{t+1}|\emph{S}_\emph{1},\emph{S}_\emph{2},...,\emph{S}_\emph{t})
$$

此外，强化学习过程中还设置了奖励函数$\emph{R}$以及定义了$\emph{t}$时刻根据某一决策获得的累积奖励$\emph{G}_\emph{t}$：

$$
\emph{G}_\emph{t} = \emph{R}_ \emph{t+1}+\gamma * \emph{R}_ \emph{t+2}+...+\gamma^k * \emph{R}_ \emph{t+k+1}
$$

当然，在时刻$\emph{t}$可能会有多种决策，计算不同决策下产生的$\emph{G}_\emph{t}$，得到一个期望，这样就得出马尔科夫决策过程中的**状态-价值函数**，即：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{G}_\emph{t}|\emph{S}_\emph{t} = \emph{s}]
$$

将$\emph{V}_\emph{s}$进一步展开，可以发现：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{R}_ \emph{t+1}] + \gamma *  \emph{E}[\emph{G}_\emph{t+1}|\emph{S}_\emph{t+1} = \emph{s}']
$$

需要注意的是，$\emph{s}'$可能有多种情况，所以公式可改写为：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{R}_ \emph{t+1}] + \gamma *  \sum_{\emph{s}' \in \emph{S}} \emph{P}_{\emph{s}\emph{s}'}*\emph{V}_{\emph{s}'} 
$$

在此基础之上，我们可以进一步进行改写，并给定决策过程$\pi$，得到**状态-价值函数**与**动作-价值函数**之间的关系：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)\sum_r P(r|s,a)*r+\gamma*\sum_a \pi(a|s) \sum_{s'} P(s'|s,a)*V_{s'}
$$

再定义**动作-价值函数**$Q_{\pi}(s,a)$：

$$
Q_{\pi}(s,a) = \sum_r P(r|s,a)*r + \gamma* \sum_{s'} P(s'|s,a)*V_{s'}
$$

则可以得到著名的**贝尔曼方程**表达式：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)*Q_{\pi}(s,a) 
$$

不难发现，该公式表明在某一个决策过程中，特定状态的价值可由该状态下可能采取的行动价值加权得到，而行动价值又由奖励函数以及下一个可能到达的状态的价值共同确定

以上推倒过程涉及到了**奖励函数**$R$、**状态空间**$S$、**动作空间**$A$、**状态转移概率**$P(s'|s,a)$以及**折扣因子**$\gamma$，因此马尔科夫决策过程可用该五元组进行表示。

## **Q-Leanring算法**
Q-Leanring中**动作-价值函数**(Q值)更新公式需要用到贝尔曼最优方程，可理解为：**即时奖励**+折扣因子***下一个状态价值**，

$$
Q(s,a) = R(s,a)+γ*max_{a'}Q(s',a') 
$$

在q-Leanring算法中，即时奖励由预先定义好的奖励机制给出，下一个状态价值则需要通过Q-Table迭代收敛得到，由于需要打表，当**状态空间连续**时，该方法不再适用，但其Q值迭代公式仍然值得参考。

不过，利用上述更新公式可能存在更新不稳定，收敛过慢的问题，因此可以稍作改进，类似神经网络参数更新，采用类似的思想:

$$
预测新值 = 预测旧值 + 学习率*(真实值-预测旧值) 
$$

引入学习率$\alpha$后，表示如下：

$$
Q(s,a) =Q(s,a)+\alpha *(R(s,a)+γ*max_{a'}Q(s',a')-Q(s,a))
$$


## **DQN算法**
DQN算法是对Q-Learning算法的改进，其核心是使用神经网络来代替Q-Table，从而实现对Q值的迭代更新。

因此，DQN算法将原来更新Q-Table的操作替换成了更新神经网络参数，过程中采用均方误差作为损失函数，表示如下：

$$
Loss = \frac{1}{N}\sum_{i=1}^N(R(s,a)+γ*max_{a'}Q(s',a',\theta^-)-Q(s,a,\theta))^2
$$

此外，DQN算法还创新性的引入了**经验回放**以及**目标网络**的概念，

一方面，通过经验回放的随机抽取，可以打乱数据样本之间的强关联性；另一方面，若损失函数中的参数同时采用$\theta$，由于参数$\theta$每轮都会更新，这会导致神经网络学习不到趋势而使训练过程不稳定，引入目标网络$\theta^-$可以较好保证收敛性。

DQN可以很好地解决连续状态空间的问题，但不能处理连续动作空间，因为DQN网络输出的是一个Q值，当动作空间离散时，尚可通过比较取最大值；但当动作空间为连续时，无法通过比较取最大值。

## **Policy-Gradient 算法**
前面两个算法都是基于**动作-价值函数**，且都无法解决连续动作空间问题。Policy-Gradient可以解决这个问题，其基于**状态-价值函数**(V)，旨在找到一个策略，使得策略的奖励期望值最大。

一般地，假设有一条轨迹$\tau$：

$
\{s_1,a_1,s_2,a_2,s_3,a_3,...,s_t,a_t\}
$

则其在策略$\pi$下发生的概率为：

$
\begin{aligned}
p_{\theta}(\tau) &= p\left(s_{1}\right) p_{\theta}\left(a_{1} \mid s_{1}\right) p\left(s_{2} \mid s_{1}, a_{1}\right) p_{\theta}\left(a_{2} \mid s_{2}\right) p\left(s_{3} \mid s_{2}, a_{2}\right) \cdots \\
&= p\left(s_{1}\right) \prod_{t=1}^{T} p_{\theta}\left(a_{t} \mid s_{t}\right) p\left(s_{t+1} \mid s_{t}, a_{t}\right)
\end{aligned}
$

Policy-Gradient算法的目标是最大化奖励期望，因此需要求奖励函数对神经网络参数($\theta$)的偏导，即：

$$
\begin{aligned}
\nabla \bar{R}_{\theta} &= \sum_{\tau} R(\tau) \nabla p_{\theta}(\tau) \\
&= \sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\
&= \sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
&= \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ R(\tau) \nabla \log p_{\theta}(\tau) \right]
\end{aligned}
$$

接着推导，可以得到：

$$
\begin{aligned}
\nabla \bar{R}_{\theta} &\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^{n}) \nabla \log p_{\theta}(\tau^{n}) \\
&= \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R(\tau^{n}) \nabla \log p_{\theta}(a_{t}^{n} \mid s_{t}^{n})
\end{aligned}
$$

其中第一步转化通过**蒙特卡洛采样**N条轨迹得到，第二步根据之前的轨迹概率公式得到。到这一步，我们就可以将该公式**取负**作为损失函数，用神经网络进行训练。

对于累计奖励$R(\tau^{n})$，有不同的计算方式，一种就是简单的将每个时间步中的$R(\tau^{n})$看成定值，即该条轨迹得到的总奖励；**REINFORCE算法**对此进行优化，其单独计算每个时间步奖励，公式如下：

$
R(\tau^{n}_{t}) = \sum_{t'=t}^{T} \gamma^{t' - t} r_{t'} 
$
：动作 $a_t$ 之后的回报；

对于$\nabla \log p_{\theta}(a_{t}^{n} \mid s_{t}^{n})$的计算方式，具体而言，首先通过网络前向传播得到动作 $a_t^n$ 的概率（或概率密度），计算其自然对数 $\log \pi_\theta(a_t^n \mid s_t^n)$，随后通过反向传播算法，对该对数概率关于网络参数 $\theta$ 求梯度 $\nabla_\theta \log \pi_\theta(a_t^n \mid s_t^n)$。

一般情况下，每得到一条完整轨迹，我们就能进行一轮更新；也可以连续得到N条轨迹后，再进行一次更新，这样可以保证收敛更加稳定。

可以发现，基于Policy-Gradient的方法只接收状态输入，对于离散动作空间，可以输出每个动作选择的概率；对于连续动作空间，可以输出采取动作的均值和方差。因此**基于策略的方法**可以很好解决连续动作空间的问题。

同时注意到，该类算法和之前**基于价值的方法**在网络更新上有所区别：前者需要一轮和环境交互的完整轨迹才能更新参数，即回合制；后者每与环境交互一次，就能更新一次参数。

## **Actor-Critic 算法**
根据贝尔曼方程，我们可以得知，算法目标需要使其**最大化**：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)*Q_{\pi}(s,a) 
$$

在Policy-Gradient 算法中，对$Q_{\pi}(s,a)$的处理是采样一整轮轨迹数据，然后据此计算。因此基于策略的方法无法在每次交互后立刻进行更新。

受基于价值函数的算法启发，考虑用神经网络近似估计$Q_{\pi}(s,a)$，另一个神经网络仍然按照Policy-Gradient算法进行训练，来近似$\pi(a|s)$。这样同时结合了基于价值和策略的方法，同时可以在每轮交互后进行更新。

AC算法每部分的逻辑基本保持不变，具体流程如下：
  
- **初始化策略网络**，根据初始状态 $s$ 选取动作 $a$，与环境交互得到 $s'$
- **计算** $Q(s,a)$
- **计算即时奖励** $r$，随后策略网络根据输入 $s'$ 选择下一步动作 $a'$
- **计算** $Q(s',a')$
- **计算TD误差**：
  $$
  \delta = r + \gamma \cdot Q(s',a') - Q(s,a)
  $$
- **根据MSE损失函数更新价值网络参数**：
  $$
  \text{Loss} = \frac{1}{2} \left( Q(s,a) - (r + \gamma \cdot Q(s',a')) \right)^2
  $$
- **根据策略梯度更新策略网络参数**，这里可利用蒙特卡洛估计，直接根据采取动作计算，不用求期望，同时Q(s,a)可替换为$\delta$，收敛更快(方差减小)：
  $$
  \nabla_\theta V(\theta) = \mathbb{E} \left[ \nabla_\theta \log \pi(a|s;\theta) \cdot Q(s,a) \right]
  $$
