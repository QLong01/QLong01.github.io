# **强化学习笔记**

> Writted on 250227

> Author:Jack_Hui

## **强化学习概述**
强化学习属于机器学习的一种。机器学习可以大体分为监督学习、无监督学习、强化学习以及深度学习四类。对于监督学习，训练前需要给出标注好的数据，比如在分类任务中，训练数据集需要给出每个图片对应的分类；对于无监督学习，给出的数据集并不需要进行标注，由机器按照特定算法总结特征，可理解为自学成才。而强化学习重点聚焦于与环境的交互过程，训练数据集并没有提前人为给出，而是通过与环境的一系列交互获得，再根据人为设置的奖励机制决定交互过程。最后深度学习涉猎最广泛，可以与前三者结合，在不同的领域上发挥更好的效果。

## **马尔科夫决策过程**
通常情况下可以使用马尔科夫决策过程对强化学习场景进行建模，该决策的一个重要特质是未来状态只受当前状态影响，公式表示如下：

$$
\emph{P}(\emph{S}_\emph{t+1}|\emph{S}_\emph{t}) = \emph{P}(\emph{S}_\emph{t+1}|\emph{S}_\emph{1},\emph{S}_\emph{2},...,\emph{S}_\emph{t})
$$

此外，强化学习过程中还设置了奖励函数$\emph{R}$以及定义了$\emph{t}$时刻根据某一决策获得的累积奖励$\emph{G}_\emph{t}$：

$$
\emph{G}_\emph{t} = \emph{R}_ \emph{t+1}+\gamma * \emph{R}_ \emph{t+2}+...+\gamma^k * \emph{R}_ \emph{t+k+1}
$$

当然，在时刻$\emph{t}$可能会有多种决策，计算不同决策下产生的$\emph{G}_\emph{t}$，得到一个期望，这样就得出马尔科夫决策过程中的**状态-价值函数**，即：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{G}_\emph{t}|\emph{S}_\emph{t} = \emph{s}]
$$

将$\emph{V}_\emph{s}$进一步展开，可以发现：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{R}_ \emph{t+1}] + \gamma *  \emph{E}[\emph{G}_\emph{t+1}|\emph{S}_\emph{t+1} = \emph{s}']
$$

需要注意的是，$\emph{s}'$可能有多种情况，所以公式可改写为：

$$
\emph{V}_\emph{s} = \emph{E}[\emph{R}_ \emph{t+1}] + \gamma *  \sum_{\emph{s}' \in \emph{S}} \emph{P}_{\emph{s}\emph{s}'}*\emph{V}_{\emph{s}'} 
$$

在此基础之上，我们可以进一步进行改写，并给定决策过程$\pi$，得到**状态-价值函数**与**行为-价值函数**之间的关系：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)\sum_r P(r|s,a)*r+\gamma*\sum_a \pi(a|s) \sum_{s'} P(s'|s,a)*V_{s'}
$$

再定义**行为-价值函数**$Q_{\pi}(s,a)$：

$$
Q_{\pi}(s,a) = \sum_r P(r|s,a)*r + \gamma* \sum_{s'} P(s'|s,a)*V_{s'}
$$

则可以得到著名的**贝尔曼方程**表达式：

$$
V^{\pi}_s = \sum_{a}\pi(a|s)*Q_{\pi}(s,a) 
$$

不难发现，该公式表明在某一个决策过程中，特定状态的价值可由该状态下可能采取的行动价值加权得到，而行动价值又由奖励函数以及下一个可能到达的状态的价值共同确定

以上推倒过程涉及到了**奖励函数**$R$、**状态空间**$S$、**动作空间**$A$、**状态转移概率**$P(s'|s,a)$以及**折扣因子**$\gamma$，因此马尔科夫决策过程可用该五元组进行表示。

## **q-Leanring算法**

## **DQN算法**